"""run_tclassify.py

Script that lets you run the classifier on a dataset.

Usage:

   $ python run_tclassify.py --classify OPTIONS
   $ python run_tclassify.py --evaluate OPTIONS
   $ python run_tclassify.py --show-data --corpus PATH
   $ python run_tclassify.py --show-pipeline --corpus PATH

For the first form, we have the following options:

   --corpus PATH - corpus directory

   --pipeline FILENAME - file with pipeline configuration; this is used to pick
        out the data set created with that pipeline; the file is assumed to be in
        the config directory of the corpus; the default is 'pipeline-default.txt'.

   --filelist FILENAME - contains files to process, that is, the elements from
        the data set used to create the model; this is an absolute or relative
        path to a file; the default is to use files.txt in CORPUS_PATH/config

   --xval INTEGER - cross-validation setting for classifier, if set to 0 (which
        is the default) then no cross-validation will be performed

   --model FILENAME - selects the model used for classification

   --output DIRECTORY - the directory where all data will be written to

   --verbose - switches on verbose mode

For the second form, we assume an existing classification and compare it to a
gold standard. The first three options are required, the fourth is optional:

   --eval-id - an identifier that is added to the output files

   --output DIRECTORY - name of the output directory created with the --classify
       option, this is also the directory where all evaluation data will be
       written to.

   --gold-standard FILENAME - file with labeled terms for evaluations

   --filter FILENAME - a file with terms to be ignored in the evaluation,
       typically the file with annotated terms for creating the training data

There are two forms that are there purely to print information about the corpus:

  --show-data        print available datasets, then exit
  --show-pipelines   print defined pipelines, then exit

Both these options require the --corpus option but nothing else, all other
options handed in will be ignored.


Examples.

For running the classifier on a corpus, you just pick your corpus and a model
and run the classifier on a set of files defined by a pipeline and a file list
(where the first defaults to the default pipeline and the second to the default
file list of the corpus). Here is a typical invocation:

   $ python run_tclassify.py \
     --classify \
     --corpus ../creation/data/patents/201312-en-500/ \
     --filelist ../creation/data/patents/201312-en-500/config/files-010.txt \
     --model data/models/technologies-201312-en-500-010/train.ds0005.standard.model \
     --output data/classifications/test2 \
     --verbose

For evaluation of the above-created classification, simply read the output and
compare it to a gold standard:

   $ python run_tclassify.py \
     --evaluate \
     --output data/classifications/test2 \
     --gold-standard ../annotation/en/technology/phr_occ.eval.lab \
     --filter ../annotation/en/technology/gold-training.txt \
     --eval-id phr_occ_eval \
     --verbose

The system will select classify.MaxEnt.out.s4.scores.sum.nr in the selected
output and consider that file to be the system response. Ideally, the gold
standard was manually created over the same files as the one in the output. When
--filter is used only terms that do not occur in the gold-training file are
evaluated.

You can also run the classifier on a list of files. Note that these files have
to be feature files, which in a corpus would reside in the data/d3_phr_feats
directory.

   $ python run_tclassify.py \
     --classify \
     --model data/models/technologies-201312-en-500-010/train.ds0005.standard.model \
     --filelist lists/list-500.txt \
     --output data/classifications/test3 \
     --verbose

What happens is similar to what happend with the corpus-based example above. The
main difference is that with the corpus example a file list with the appropriate
feature files was generated by the code.

"""

import os, sys, shutil, getopt, subprocess, codecs, time

import config
import evaluation
import train
import mallet

sys.path.append(os.path.abspath('../..'))

from ontology.classifier.utils.find_mallet_field_value_column import find_column
from ontology.classifier.utils.sum_scores import sum_scores
from ontology.utils.batch import RuntimeConfig, show_datasets, show_pipelines
from ontology.utils.batch import find_input_dataset, check_file_availability, Profiler
from ontology.utils.file import filename_generator, ensure_path, open_output_file, compress
from ontology.utils.git import get_git_commit


VERBOSE = False


class TrainerClassifier(object):

    """Abstract class with some common methods for the trainer and the
    classifier."""
    
    def _find_datasets(self):
        """Select data sets and check whether all files are available."""
        self.input_dataset = find_input_dataset(self.rconfig, 'd3_phr_feats')
        check_file_availability(self.input_dataset, self.file_list)

    def _find_filelist(self, file_list):
        if os.path.exists(file_list):
            return file_list
        return os.path.join(self.rconfig.config_dir, file_list)


class Classifier(TrainerClassifier):

    def __init__(self, rconfig, file_list, model, classifier_type, output, use_all_chunks_p):

        self.rconfig = rconfig
        self.file_list = file_list
        self.model = model
        self.classifier = classifier_type
        self.output = output
        self.use_all_chunks_p = use_all_chunks_p
        self.input_dataset = None

        self.mallet_file = self.output + os.sep + 'classify.mallet'
        self.info_file_general = os.path.join(self.output, "classify.info.general.txt")
        self.info_file_config = os.path.join(self.output, "classify.info.config.txt")
        self.info_file_filelist = os.path.join(self.output, "classify.info.filelist.txt")

        self.results_file = os.path.join(self.output, "classify.%s.out" % (self.classifier))
        self.stderr_file = os.path.join(self.output, "classify.%s.stderr" % (self.classifier))

        base = os.path.join(self.output, "classify.%s.out" % (self.classifier))
        self.classifier_output = base
        self.scores_s1 = base + ".s1.all_scores"
        self.scores_s2 = base + ".s2.y_scores"
        self.scores_s3 = base + ".s3.scores.sum"
        self.scores_s4_nr = base + ".s4.scores.sum.nr"
        self.scores_s4_az = base + ".s4.scores.sum.az"

    def run_on_corpus(self):
        self._check_output()
        t1 = time.time()
        self._find_datasets()
        self.run_classifier(t1, corpus=True)

    def run_on_files(self):
        self._check_output()
        t1 = time.time()
        self.run_classifier(t1, corpus=False)

    def run_classifier(self, t1, corpus):
        ensure_path(self.output)
        self._create_mallet_file(corpus=corpus)
        self._run_classifier()
        self._calculate_scores()
        self._create_info_files(t1)
        for fname in (self.results_file, self.mallet_file, self.scores_s1):
            print "[Classifier.run_classifier] Compressing", fname
            compress(fname)

    def _check_output(self):
        if os.path.exists(self.info_file_general):
            sys.exit("WARNING: already have classifier results in %s" % self.output)

    
    def _calculate_scores(self):
        """Use the clasifier output files to generate a sorted list of technology terms
        with their probabilities. This is an alternative way of using the commands in
        patent_tech_scores.sh."""
        self._scores_s1_select_score_lines()
        self._scores_s2_select_scores()
        self._scores_s3_summing_scores()
        self._scores_s4_sort_scores()

    def run_score_command(self, command, message):
        if VERBOSE:
            print "[--scores]", message
            print "[--scores]", command
        subprocess.call(command, shell=True)

    def _scores_s1_select_score_lines(self):
        message = "select the line from the classifier output that contains the scores"
        command = "cat %s | egrep '^[0-9]' > %s" % (self.classifier_output, self.scores_s1)
        self.run_score_command(command, message)

    def _scores_s2_select_scores(self):
        column = find_column(self.scores_s1, 'y')
        if VERBOSE:
            print "[--scores] select 'y' score from column %s of %s" % \
                  (column, os.path.basename(self.scores_s1))
        fh_in = codecs.open(self.scores_s1)
        fh_out = codecs.open(self.scores_s2, 'w')
        for line in fh_in:
            fields = line.split()
            id = fields[0]
            score = float(fields[int(column)-1])
            fh_out.write("%s\t%.6f\n" % (id, score))

    def _scores_s3_summing_scores(self):
        if VERBOSE:
            print "[--scores] summing scores into", os.path.basename(self.scores_s3)
        sum_scores(self.scores_s2, self.scores_s3)

    def _scores_s4_sort_scores(self):
        message1 = "sort on average scores"
        message2 = "sort on terms"
        command1 = "cat %s | sort -T data/tmp -k2,2 -nr -t\"\t\" > %s" \
                   % (self.scores_s3, self.scores_s4_nr)
        command2 = "cat %s | sort -T data/tmp > %s" % (self.scores_s3, self.scores_s4_az)
        self.run_score_command(command1, message1)
        self.run_score_command(command2, message2)

    def _create_info_files(self, t1):
        with open(self.info_file_general, 'w') as fh:
            fh.write("$ python %s\n\n" % ' '.join(sys.argv))
            fh.write("output           =  %s\n" % os.path.abspath(self.output))
            fh.write("file_list        =  %s\n" % self.file_list)
            fh.write("model            =  %s\n" % self.model)
            fh.write("features         =  %s\n" % ' '.join(self.features))
            fh.write("config_file      =  %s\n" % rconfig.pipeline_config_file)
            fh.write("timestamp        =  %s\n" % time.strftime("%Y%m%d:%H%M%S"))
            fh.write("processing time  =  %ds\n" % int(time.time() - t1))
            fh.write("git_commit       =  %s" % get_git_commit())
        if self.rconfig.pipeline_config_file is not None:
            shutil.copyfile(self.rconfig.pipeline_config_file, self.info_file_config)
        shutil.copyfile(self.file_list, self.info_file_filelist)

    def _create_mallet_file(self, corpus=True):
        if corpus:
            fnames = filename_generator(self.input_dataset.path, self.file_list)
        else:
            fnames = [f.strip() for f in open(self.file_list).readlines()]
            fnames = [f for f in fnames if f.strip()]
        fh = open_output_file(self.mallet_file, compress=False)
        self._set_features()
        if VERBOSE:
            print "[create_mallet_file] features: %s" % (self.features)
        features = dict([(f,True) for f in self.features])
        stats = { 'labeled_count': 0, 'unlabeled_count': 0, 'total_count': 0 }
        count = 0
        for fname in fnames:
            count += 1
            if VERBOSE:
                print "[create_mallet_file] %05d %s" % (count, fname)
            train.add_file_to_utraining_test_file(
                fname, fh, {}, features, stats, use_all_chunks_p=self.use_all_chunks_p)
        fh.close()
        if VERBOSE:
            print "[create_mallet_file]", stats

    def _run_classifier(self):
        mclassifier = mallet.SimpleMalletClassifier(
            config.MALLET_DIR,
            classifier_type=self.classifier)
        mclassifier.run_classifier(
            self.model,
            self.mallet_file,
            self.results_file,
            self.stderr_file)

    def _set_features(self):
        # TODO. It is a bit unclear now how we get the features, in the past
        # they came from the model.info file, recursing to parent info files if
        # needed, then we found them in the mallet.info file, and now we find
        # them in info/train.info.general.txt. The question is whether the
        # features are always there and if they are the correct ones in case we
        # selected features twice; maybe never allow feature selection to happen
        # twice.
        if VERBOSE:
            print "[get_features] model file =", self.model
        info_file = os.path.splitext(self.model)[0] + '.mallet.info'
        if not os.path.exists(info_file):
            model_dir = os.path.dirname(self.model)
            info_file = os.path.join(model_dir, 'info', 'train.info.general.txt')
        features = parse_info_file(info_file)
        if features.has_key('features'):
            feature_set = frozenset(features['features'].split())
            self.features = sorted(list(feature_set))
        else:
            print "[_set_features] WARNING: no features found, using all.features."
            self.features = sorted(get_features())


def parse_info_file(fname):
    """Parse an info file and return a dictionary of features. Return None if the
    file does not exist. Assumes that the last feature of note is always
    git_commit."""
    # TODO: why is git_commit supposed to be the last feature?
    if VERBOSE:
        print "[parse_info_file]", fname
    features = {}
    try:
        for line in open(fname):
            if line.find('=') > -1:
                f, v = line.split('=', 1)
                features[f.strip()] = v.strip()
                if f.strip() == 'git_commit':
                    break
    except IOError:
        print "[parse_info_file] WARNING: no such file '%s'" % fname
    return features

def get_features():
    """Returns the list of features in the all.features file."""
    # TODO: this is a copy of a method in create_mallet_file (except that the
    # doc string is very different), consolidate/refactor these two
    script_dir = os.path.dirname(os.path.abspath(sys.argv[0]))
    features_file = os.path.join(script_dir, "features", "all.features")
    content = open(features_file).read().strip()
    return content.split()


def evaluate(output, gold_standard, tfilter, id):
    """Evaluate results in output given a gold standard. It is the responsibility
    of the user to make sure that it makes sense to compare this gold standard
    to the system results."""
    print "Evaluating system results in %s" % output
    system_file = os.path.join(output, 'classify.MaxEnt.out.s4.scores.sum.nr')
    command =  "python %s" % ' '.join(sys.argv)
    for term_type in ('all', 'single-token-terms', 'multi-token-terms'):
        ttstring = term_type_as_short_string(term_type)
        tfstring = term_filter_as_short_string(tfilter)
        summary_file = os.path.join(output, "eval-%s-%s-%s.txt" % (id, ttstring, tfstring))
        summary_fh = codecs.open(summary_file, 'w', encoding='utf-8')
        for threshold in (0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9):
            if threshold == 0.5:
                log_file = os.path.join(output, "eval-%s-%s-%s-%.1f.txt" \
                                        % (id, ttstring, tfstring, threshold))
            else:
                log_file = None
            result = evaluation.test(gold_standard, system_file, threshold, log_file,
                                     term_type=term_type, term_filter=tfilter,
                                     debug_c=False, command=command)
            summary_fh.write(result)


def term_type_as_short_string(term_type):
    if term_type == 'all': return 'all'
    if term_type == 'single-token-terms': return 'stt'
    if term_type == 'multi-token-terms': return 'mtt'

def term_filter_as_short_string(term_filter):
    return 'ntf' if term_filter is None else 'ytf'


def read_opts():
    longopts = ['classify', 'evaluate', 'show-data', 'show-pipelines',
                'corpus=', 'language=', 'pipeline=', 'filelist=',
                'mallet-dir=', 
                'output=', 'features=', 'xval=', 'model=', 'eval-on-unseen-terms',
                'verbose', 'eval-id=', 'gold-standard=', 'threshold=', 'filter=', 'logfile=']
    try:
        return getopt.getopt(sys.argv[1:], 'c:m:b:f:v', longopts)
    except getopt.GetoptError as e:
        sys.exit("ERROR: " + str(e))



if __name__ == '__main__':

    # default values of options
    corpus_path = None
    file_list = 'files.txt'
    pipeline_config = 'pipeline-default.txt'
    classify_p, evaluate_p = False, False
    show_data_p, show_pipelines_p = False, False
    model, output, xval, = None, None, "0"
    use_all_chunks = True
    eval_id = None
    gold_standard = None
    filter_terms = None
    threshold = None

    (opts, args) = read_opts()

    for opt, val in opts:

        if opt == '--evaluate': evaluate_p = True
        elif opt == '--classify': classify_p = True
        elif opt == '--show-data': show_data_p = True
        elif opt == '--show-pipelines': show_pipelines_p = True
        elif opt == '--mallet-dir':
            if os.path.isdir(val):
                config.MALLET_DIR = val
            else:
                exit("WARNING: non-existing directory for --mallet-dir")

        elif opt in ['-c', '--corpus']: corpus_path = val
        elif opt in ['-m', '--model']: model = val
        elif opt in ['-b', '--output']: output = val
        elif opt in ['-f', '--filelist']: file_list = val

        elif opt == '--pipeline': pipeline_config = val
        elif opt == '--xval': xval = val

        elif opt == '--eval-id': eval_id = val
        elif opt == '--gold-standard': gold_standard = val
        elif opt == '--threshold': threshold = float(val)
        elif opt == '--filter': filter_terms = val

        elif opt in ['-v', '--verbose']: VERBOSE = True
        elif opt == '--eval-on-unseen-terms': use_all_chunks = False

    # there is no language to hand in to the runtime config, but it will be
    # plucked from the general configuration if needed
    if not evaluate_p:
        rconfig = RuntimeConfig(corpus_path, model, output, None, None, pipeline_config)

    if show_data_p and corpus_path:
        show_datasets(rconfig, config.DATA_TYPES, VERBOSE)

    elif show_pipelines_p and corpus_path:
        show_pipelines(rconfig)

    elif evaluate_p:
        evaluate(output, gold_standard, filter_terms, eval_id)

    elif classify_p:
        if VERBOSE: rconfig.pp()
        # allow for the file_list to be just the filename in the config
        # directory of the corpus
        if model is None:
            exit("ERROR: no model given")
        if not os.path.exists(model):
            exit("ERROR: model does not exist: '%s'" % model)
        if not os.path.exists(file_list):
            file_list = os.path.join(corpus_path, 'config', file_list)
        # TODO: we now just hand in MaxEnt as the classifier type because that
        # is what we always use, but really the model info should store the
        # classifier type selected and the classifier should just use that
        classifier = Classifier(rconfig, file_list, model, 'MaxEnt', output,
                                use_all_chunks_p=use_all_chunks)
        if corpus_path is not None:
            classifier.run_on_corpus()
        else:
            classifier.run_on_files()

    else:
        print "\nWARNING: cannot run classifier\n"
        print "Usage:"
        print "   $ python run_tclassify.py --classify OPTIONS"
        print "   $ python run_tclassify.py --evaluate OPTIONS"
        print "   $ python run_tclassify.py --show-data --corpus PATH"
        print "   $ python run_tclassify.py --show-pipeline --corpus PATH"
